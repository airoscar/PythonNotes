{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('crypto_data/LTC-USD.csv', names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a dataframe by combining the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "ratios = ['BTC-USD','LTC-USD','ETH-USD','BCH-USD']\n",
    "\n",
    "for ratio in ratios:\n",
    "    \n",
    "    dataset = f'crypto_data/{ratio}.csv'\n",
    "    df=pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "    \n",
    "    df.rename(columns={'close': f\"{ratio}_close\", 'volume':f\"{ratio}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df.loc[:, [f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    main_df=pd.merge(main_df, df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new target column which indicates whether price increases or decreases in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60     # number of minutes from the past used for prediction\n",
    "FUTURE_PERIOD_PREDICT = 3    # number of minutes into the future for prediction\n",
    "RATIO_TO_PREDICT = 'LTC-USD' # item to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# create a new 'future' column which shows price values 3 minutes from the timestamp\n",
    "main_df['future'] = main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "\n",
    "# creates a target column, which contains boolean values indicating whether future is higher\n",
    "main_df['target']=list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df['future']))\n",
    "\n",
    "# Using time as index and sort by time\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "\n",
    "# Slice out the last 5 percent of data (in order of time) as validation data\n",
    "validation_main_df = main_df[main_df.index >= last_5pct]\n",
    "main_df = main_df[main_df.index < last_5pct]\n",
    "\n",
    "print(main_df.shape, validation_main_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sequence of the data: ie each data point would be the past 60 minutes of price data.\n",
    "#### Balancing the data to make sure classes have equal amounts when training (so the model doesnt always just predict the class with highest occurance in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_df(df):\n",
    "    '''Takes a Pandas dataframe as parameter.\n",
    "        Remove a column named \"future\"\n",
    "        Change columns into percent change values\n",
    "        Produces sequential data\n",
    "        Balance the sequential data\n",
    "        Return X and y as feature matrix and targets'''\n",
    "    \n",
    "    df = df.drop('future', axis=1)    # 'future' column no longer needed\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            df[col] = df[col].pct_change()  # change values to percent change from previous values\n",
    "            df[col] = preprocessing.scale(df[col].values)  # rescale the value to 0-1\n",
    "    \n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "        \n",
    "    sequential_data=[]\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:              # each i represents a row of values\n",
    "        prev_days.append(i[:-1])     # append everything except for the last 'target' value\n",
    "        \n",
    "        # produces a sequence of data using the time interval specified\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]]) \n",
    "            # append features, target \n",
    "            # Note that the prev_days has features of the previous time intervals, but only the target of the current time stamp\n",
    "            \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    # Based on the target value, split training data into buys and sells\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "            \n",
    "    # balance the buys and sells\n",
    "    lower = min(len(buys), len(sells))\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    # re-combine the balanced data and shuffle\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_df(main_df)\n",
    "X_test, y_test = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"train data: {len(X_train)}, test data: {len(X_test)}\")\n",
    "print(f\"Training Targets: buys: {y_train.count(1)}, sells: {y_train.count(0)}\")\n",
    "print(f\"Test Targets: buys: {y_test.count(1)}, sells: {y_test.count(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{RATIO_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using NVDA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using AMD GPU (With plaidml installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='plaidml.keras.backend'\n",
    "# When using plaidml, the libraries are imported from keras instead of tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\" # adds validation accuracy into the filename for each epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[tensorboard, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "model.save(\"models/{NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
