{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 521240\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark Job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted cluster 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark Context\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"60:00\" #60 min \n",
    "os.environ['SBATCH_PARTITION']='single' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Job')\n",
    "    sj.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sj.wait_to_start()\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Context')\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "|index|age|grade|\n",
      "+-----+---+-----+\n",
      "|    1|  2|    3|\n",
      "|    5|  6|    7|\n",
      "+-----+---+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "df = spark.read.csv('./TestCSV.csv',inferSchema=True,header=True)\n",
    "\n",
    "print(df.show(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|    1|\n",
      "|    5|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.select(\"index\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mike', 18.5], ['sara', 20.5], ['alex', 15.0]]\n"
     ]
    }
   ],
   "source": [
    "myInfo=sc.textFile(\"./name-age.csv\")\n",
    "data = myInfo.map(lambda x: x.split(\",\"))\n",
    "aggregatedData=data.map(lambda x: (x[0], [int(x[1]),1])).reduceByKey(lambda x,y:[x[0]+y[0],x[1]+y[1]]).map(lambda x:[x[0],x[1][0]/x[1][1]])\n",
    "print(aggregatedData.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='alex', avg(age)=15.0), Row(name='sara', avg(age)=20.5), Row(name='mike', avg(age)=18.5)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import col, avg\n",
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "df = spark.read.csv('./name-age-Header.csv',inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "aggregatedData2=sqlCtx.table(\"people\").groupBy(\"name\").agg({\"age\": \"avg\"}).collect()\n",
    "print(aggregatedData2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='alex', avg(age)=15.0), Row(name='sara', avg(age)=20.5), Row(name='mike', avg(age)=18.5)]\n"
     ]
    }
   ],
   "source": [
    "aggregatedData2=sqlCtx.table(\"people\").groupBy(\"name\").agg({\"age\": \"avg\"}).collect()\n",
    "print(aggregatedData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='sara', avg(age)=20.5, min(age)=15), Row(name='mike', avg(age)=18.5, min(age)=12)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, min\n",
    "aggregatedData3=sqlCtx.table(\"people\").groupBy(\"name\").agg(avg(\"age\"),min(\"age\")).where(avg(\"age\") >15).collect()\n",
    "print(aggregatedData3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='mike', age=25), Row(name='sara', age=26), Row(name='alex', age=15), Row(name='mike', age=12), Row(name='sara', age=15)]\n"
     ]
    }
   ],
   "source": [
    "aggregatedData4=sqlCtx.sql(\"select * from people\")\n",
    "print(aggregatedData4.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=25, name='mike'), Row(age=26, name='sara'), Row(age=15, name='alex'), Row(age=12, name='mike'), Row(age=15, name='sara')]\n",
      "[Row(age=25), Row(age=26), Row(age=15), Row(age=12), Row(age=15)]\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"age\",\"name\").collect())\n",
    "\n",
    "print(df.select(df[\"age\"]).collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='alex', age=15), Row(name='sara', age=15)]\n"
     ]
    }
   ],
   "source": [
    "print(df.where((df.age == 15) & (df.age>10)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='mike', course='cpsc 447'), Row(name='sara', course='ensf 619'), Row(name='alex', course='stat 213'), Row(name='mike', course='ensf 619.28'), Row(name='sara', course='ensf 19.25')]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "coursesDf = spark.read.csv('./name-course.csv',inferSchema=True,header=True)\n",
    "coursesDf.createOrReplaceTempView(\"courses\")\n",
    "\n",
    "print(coursesDf.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='mike', age=12, name='mike', course='cpsc 447'), Row(name='mike', age=25, name='mike', course='cpsc 447'), Row(name='sara', age=15, name='sara', course='ensf 619'), Row(name='sara', age=26, name='sara', course='ensf 619'), Row(name='alex', age=15, name='alex', course='stat 213'), Row(name='mike', age=12, name='mike', course='ensf 619.28'), Row(name='mike', age=25, name='mike', course='ensf 619.28'), Row(name='sara', age=15, name='sara', course='ensf 19.25'), Row(name='sara', age=26, name='sara', course='ensf 19.25')]\n"
     ]
    }
   ],
   "source": [
    "print(df.join(coursesDf , df.name==coursesDf.name, 'inner').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
